{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8aca8fa-191d-4294-b199-b7fda4d55b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import time\n",
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2000b3c6-1f48-4bf5-89bc-9b5464028b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "dmodel = 512\n",
    "heads = 4\n",
    "batch_size = 128\n",
    "max_len = 100\n",
    "encoded_sentence = torch.randn((batch_size,max_len,dmodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2738ce16-3467-4300-b3bb-92442eadaca1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(encoded_sentence,shape):\n",
    "    '''\n",
    "    Converts the vector embedding of batch of sequence to their positional encoding vectors.\n",
    "\n",
    "    Arguments:\n",
    "            encoded_sentence : embbeding vector which is to be Positional Encoded.\n",
    "            shape : shape of embbeding vector => tuple(batch_size,max_len,dmodel)\n",
    "            \n",
    "    Returns : \n",
    "            positional encoded vector \n",
    "    \n",
    "    '''\n",
    "    #shape initialization\n",
    "    max_len = shape[1]\n",
    "    dmodel = shape[2]\n",
    "    batch_size = shape[0]\n",
    "    \n",
    "    #create a position vector containing position of words \n",
    "    position = torch.arange(0,max_len).float().unsqueeze(1)  \n",
    "\n",
    "    #applies the formula for and creates divsion term\n",
    "    div_term = torch.exp(torch.arange(0,dmodel,2).float() * -(math.log(10000.0) / dmodel)) \n",
    "\n",
    "    #creates the zeros vector of sentence shape \n",
    "    pos_enc = torch.zeros((encoded_sentence.shape))\n",
    "\n",
    "    #applies the formula for sin(even) and cos(even)\n",
    "    pos_enc[:,:,0::2] = torch.sin(position * div_term)\n",
    "    pos_enc[:,:,1::2 ] = torch.cos(position * div_term)\n",
    "\n",
    "    #shape(batch_size,max_len,dmodel)\n",
    "    return pos_enc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a894cf5e-b070-440f-bb8b-37fd417de28a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(k,q,v):\n",
    "    '''\n",
    "    applies the attention formula for single heads \n",
    "\n",
    "    Arguments:\n",
    "            k : key\n",
    "            q : query\n",
    "            v : value)\n",
    "    Returns : \n",
    "            single matrix same as shape of k,q,v\n",
    "    '''\n",
    "    return torch.matmul(F.softmax((torch.matmul(q,k.transpose(-1,-2)))/(torch.sqrt(torch.tensor(dmodel/heads))),dim=-1) , v)\n",
    "    \n",
    "def multi_headed_attention(K,Q,V,heads):\n",
    "    \n",
    "    '''\n",
    "    applies multi headed attention\n",
    "\n",
    "    Arguments:\n",
    "            K : key\n",
    "            Q : query\n",
    "            V : value\n",
    "    Returns : \n",
    "            matrix of shape(K) after applying  multi headed attention      \n",
    "    '''\n",
    "\n",
    "    head_size = int(dmodel/heads)\n",
    "    Wk = torch.randn((dmodel,dmodel))\n",
    "    Wv = torch.randn((dmodel,dmodel))\n",
    "    Wq = torch.randn((dmodel,dmodel))\n",
    "\n",
    "    # shape(batch_size,max_len,dmodel) \n",
    "    K_prime = K @ Wk\n",
    "    Q_prime = Q @ Wq\n",
    "    V_prime = V @ Wv\n",
    "\n",
    "    #split into multi heads \n",
    "    def split_heads(matrix,shape):\n",
    "        return matrix.view(*shape)\n",
    "        \n",
    "    #defines the shape of multiheaded matrix     \n",
    "    shape = (K.shape[0],K.shape[1],heads,head_size)\n",
    "\n",
    "    #applies split head \n",
    "    K_prime = split_heads(K_prime,shape)\n",
    "    Q_prime = split_heads(Q_prime,shape)\n",
    "    V_prime = split_heads(V_prime,shape)\n",
    "\n",
    "    #applies attention and then concatinate \n",
    "    return attention(K_prime,Q_prime,V_prime).view(*K.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01b7d9c0-17ae-4bcf-b563-f95409968b98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_and_norm(positional_encoded,attention):\n",
    "    residual = positional_encoded\n",
    "    return torch.add(residual , F.layer_norm(attention,normalized_shape=(dmodel,)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1295239-be83-4925-9322-c3f3c61b4bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feed_forward_layer(matrix):\n",
    "    linear1 = nn.Linear(512,512,bias=True)\n",
    "    relu1 = nn.ReLU()\n",
    "    linear2 = nn.Linear(512,512,bias=True)\n",
    "    return linear2(relu1(linear1(matrix)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f8d21a3e-c1b8-476d-b32c-ddc0fe8cb089",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoder(X,shape,heads):\n",
    "    positional_encoded = positional_encoding(X,shape)\n",
    "    attention = multi_headed_attention(positional_encoded,positional_encoded,positional_encoded,heads)\n",
    "    add_norm1 = add_and_norm(positional_encoded,attention)\n",
    "\n",
    "    feed_forward = feed_forward_layer(add_norm1)\n",
    "    add_and_norm2 = add_and_norm(add_norm1,feed_forward)\n",
    "    return add_and_norm2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a454aa38-c5ef-45eb-bf83-5ac7166dfe01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decoder(encoded_sequence , output ,shape , heads ,output_size):\n",
    "    output_embedding = positional_encoding(output,shape)\n",
    "    attention1 = multi_headed_attention(output_embedding,output_embedding,output_embedding,heads)\n",
    "    add_and_norm1 = add_and_norm(output_embedding,attention1)\n",
    "\n",
    "    attention2 = multi_headed_attention(encoded_sequence,add_and_norm1,encoded_sequence,heads)\n",
    "    add_and_norm2 = add_and_norm(add_and_norm1,attention2)\n",
    "\n",
    "    feed_forward = feed_forward_layer(add_and_norm2)\n",
    "    add_and_norm3 = add_and_norm(add_and_norm2,feed_forward)\n",
    "\n",
    "    flattened_matrix = torch.flatten(add_and_norm3, start_dim=1, end_dim=2)\n",
    "    linear = nn.Linear(512*100,output_size)\n",
    "    softmax = F.softmax(linear(flattened_matrix),dim=1)\n",
    "    \n",
    "    return softmax\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "034ec879-6423-443f-8301-841a0d1b3804",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sequence  = encoder(encoded_sentence,(batch_size,max_len,dmodel),heads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ecf753c1-a18b-47f9-bf36-b3d7cc5917d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "output = torch.randn((batch_size,max_len,dmodel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "36bed0b6-e157-4f2c-8187-cc949ee0c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = decoder(encoded_sequence,output,(128, 100, 512),heads,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4ff77352-47a9-4728-b767-2ed16ab3babd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0020, grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder[0][886]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18de70d-6dd1-4c55-a96e-602fe6486921",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
